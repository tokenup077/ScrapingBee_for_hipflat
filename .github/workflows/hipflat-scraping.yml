# .github/workflows/hipflat-scraping.yml
name: Hipflat Scraping

on:
  schedule:
    - cron: '0 1 * * *'  # 毎日午前1時（UTC）に実行
  workflow_dispatch:    # 手動実行用トリガー

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4 pandas gspread oauth2client
        
    - name: Set up Google credentials
      run: |
        echo '${{ secrets.GOOGLE_CREDENTIALS }}' > credentials.json
      
    - name: Run scraper
      env:
        SCRAPINGBEE_API_KEY: ${{ secrets.SCRAPINGBEE_API_KEY }}
        SPREADSHEET_ID: ${{ secrets.SPREADSHEET_ID }}
        MAX_PAGES: 5
        MAX_DETAILS: 10
      run: |
        python hipflat_scrapingbee.py
        
    - name: Archive results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: scraping-results
        path: |
          *.csv
          *.log
